{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will run our baseline models to better understand our dataset and how the features and target variables behave when running different models. Our focus is to find baseline models that are able improve the Precision without affecting Recall. To evaluate the models, we will use the metrics Precison, Accuracy, Recall, F1 Score, and Confusion Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, precision_score, confusion_matrix, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import set_config\n",
    "set_config(print_changed_only=False)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "% matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ead in UCI Heart Disiease Databasae\n",
    "df = pd.read_csv('fetal_health.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fetal_health = np.where(df.fetal_health > 1.0, 2.0, df.fetal_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Objectives: </b>\n",
    "\n",
    "- Assign our feature variables and target variable into the X and y variables. \n",
    "- Split our dataset using Sklearn Train Test Split. We will use Sklearn default option and assign 75% to our train set and 25% to our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning features variables (X) and target variable (y)\n",
    "X = df.drop('fetal_health', axis=1)\n",
    "y = df.fetal_health\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Checking if train test split ran correclty\n",
    "for dataset in [y_train, y_test]:\n",
    "    print(round(len(dataset)/len(y), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train and test set were splited correctly. 75% of the dataset was assigned to our train set and and 25% was assigned to our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand our dataset and how our features variables affect our target variable, we decided to run baseline models and find what approach we should take when tuning our models. None of the baseline models will have any kinds of hyperparameters tuning. We will use the default hyperparameters.\n",
    "\n",
    "<b>Objectives:</b>\n",
    "- Run a Logistic Regression, KNN, Decision Tree, and Random Forest model.\n",
    "- Create a function to simplify our model evaluation.\n",
    "- Evaluate each model using the Accuracy, Recall, F1 Score, and Precision metrics.\n",
    "- Create confusion matrices for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "    \n",
    "# Print Accuracy, Recall, F1 Score, and Precision metrics.\n",
    "    print('Evaluation Metrics:')\n",
    "    print('Accuracy: ' + str(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('Recall: ' + str(metrics.recall_score(y_test, y_pred)))\n",
    "    print('F1 Score: ' + str(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: ' + str(metrics.precision_score(y_test, y_pred)))\n",
    "    \n",
    "# Print Confusion Matrix\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(' TN,  FP, FN, TP')\n",
    "    print(confusion_matrix(y_true, y_pred).ravel())\n",
    "    \n",
    "# Function Prints best parameters for GridSearchCV\n",
    "def print_results(results):\n",
    "    print('Best Parameters: {}\\n'.format(results.best_params_))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to classify structured data, logistic regression models usually give a quick and solid result. Thus, it will be our first baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8834586466165414\n",
      "Recall: 0.9535452322738386\n",
      "F1 Score: 0.9263657957244655\n",
      "Precision: 0.9006928406466512\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[390  19  43  80]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Logistic Regression Model\n",
    "\n",
    "lr_baseline = LogisticRegression()\n",
    "\n",
    "# Fitting and predicting\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "y_pred_lr_baseline = lr_baseline.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_lr_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findinds:</b>\n",
    "\n",
    "As we can see, our baseline Logistic Regression model performed fairly well even without tuning. Our feature variables are able to train the model with a fairly precision.\n",
    "We will now test the same features and target variables using K-nearest neighbors, which classifies data points based on a similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run a K-Nearest Neighbors model. KNN is a simples model that stores and classifies all data points based on the similarity measure (i.e. distance functions). Since it's an easy model to set up, we will run it and read the results to understand what the data tells us. We will have to scaled our dataset for KNN before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call and fit scaler \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scaling our dataset\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9398496240601504\n",
      "Recall: 0.9877750611246944\n",
      "F1 Score: 0.961904761904762\n",
      "Precision: 0.9373549883990719\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[404   5  27  96]\n"
     ]
    }
   ],
   "source": [
    "# Baseline KNN Model\n",
    "knn_baseline = KNeighborsClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "knn_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred_knn_baseline = knn_baseline.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_knn_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findinds:</b>\n",
    "\n",
    "Our baseline KNN model perfomed better than our baseline Logistic Regression model in all the metrics that we are analysing the model. Interestigly, the Recall metric had a performance of .99. This is very high. It means that our baseline model is capable of predicting true positives with high precision. For now, we will keep running baseline models to find what the dataset has to tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key questions of our project is find what metrics can predict which features have the highest coefficient in our model predictions. For this reason we will run Decision Tree as well, since it has a better division of features to predict a final outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.924812030075188\n",
      "Recall: 0.941320293398533\n",
      "F1 Score: 0.9506172839506173\n",
      "Precision: 0.9600997506234414\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[385  24  16 107]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Decision Tree Model\n",
    "dt_baseline = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "dt_baseline = tree_baseline.fit(X_train, y_train)\n",
    "y_pred_dt_baseline = tree_baseline.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_dt_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findinds:</b>\n",
    "\n",
    "Decision Tree did not perform better than KNN for most of the metrics that we are using. However, it performed better for the precision metric, which is the metric that we are most concerned about. Thus, we will run a Random Forest model, which work as some sort of colection of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a Decision Tree model, it makes sense to run a Random Forest model, which is an ensemble model that operate by constructing a multitude of decision trees at training time and outputting the class that is the average prediction for the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9548872180451128\n",
      "Recall: 0.9926650366748166\n",
      "F1 Score: 0.9712918660287081\n",
      "Precision: 0.9508196721311475\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[406   3  21 102]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Random Forest Model\n",
    "rfc_baseline = RandomForestClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "rfc_baseline.fit(X_train, y_train)\n",
    "y_pred_rfr_baseline = rfc_baseline.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_rfr_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findinds:</b>\n",
    "\n",
    "Our baseline random forest model had an interesting performance. It outperformed the Decision Tree baseline model in almost every metric, except Precision which had a slighly decrease. Compared to KNN, our best performing model so far if we compare every metric, it performed better for the metrics Accuracy, F1 Score, and Precision. It performed exactly the same for the Recall metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was our best performing metric for Accuracy, Recall, and F1 Score. Decision Tree had the best performance for the Precision metric. All of our models performed fairly well. However, we have space for improvement in all of them. We will start improving some hyperparameters and check how they performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprossessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a class imbalance problem in our dataset. Thus, we will use Synthetic Minority Oversampling Technique (SMOTE), where the minority class i oversampled by producing synthetic examples to fix the class imbalance. Then, we will compare the results to our baseline models to check if the target variable is even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    0.781681\n",
      "2.0    0.218319\n",
      "Name: fetal_health, dtype: float64\n",
      "2.0    0.5\n",
      "1.0    0.5\n",
      "Name: fetal_health, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "# Checking if SMOTE was correctly fitted\n",
    "for dataset in (y_train, y_train_smote):\n",
    "    print (dataset.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Models Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start improving our model using hyperparameters tuning without any feature engeeniring. We will tune all the baseline models changing some hyperparameters. Then, we will use GridSearchCV and XGBoost to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the first baseline model that we ran: Logistic Regression using SMOTE. We want to see if SMOTE can help our model best predict our target variable. We will not tuned any hyperparameter for the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8308270676691729\n",
      "Recall: 0.8337408312958435\n",
      "F1 Score: 0.883419689119171\n",
      "Precision: 0.9393939393939394\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[341  68  22 101]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fitting and predicting\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "y_pred_lr_smote = lr.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_lr_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to our model using SMOTE underperformed in almost every metric using SMOTE. However, the Precision metric did perform better. It's the main metric that we are interested in. We can see that there the number of False Negatives was reduced.\n",
    "\n",
    "Next, we will add a few hyperparameters individually and see if we can improve our model. Here, our approach it be trying random hyperparameter and check the results compared to the model without any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8289473684210527\n",
      "Recall: 0.8264058679706602\n",
      "F1 Score: 0.8813559322033899\n",
      "Precision: 0.9441340782122905\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[338  71  20 103]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "lr = LogisticRegression(C=100, max_iter=200, class_weight='balanced')\n",
    "\n",
    "# Fitting and predicting\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "y_pred_lr_tuned = lr.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_lr_tuned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`C=100` improved the Precision metric. All the other metrics remained the same.\n",
    "\n",
    "`fit_intercept=False` reduced all the metrics.\n",
    "\n",
    "`max_inter=200` improved the Precision metric compared to the default `max_inter=100`\n",
    "\n",
    "`class_weight='balanced'` improved all the metrics.\n",
    "\n",
    "<b>Findings:</b>\n",
    "\n",
    "Compared to our baseline model, the Logistic Regression model did not performed better. We were able to improve slightly the model using some random tuning. However, we don't think that Logistic Regression has the power that we need. Thus, we will use some more advanced algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our order of baseline models, we will know use KNN models. We will first compared the result of our baseline model to a model after applying SMOTE. Then, we will use GridSearchCV to find the best hyperparameter tuning. For this model, we will scale the train set using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7725563909774437\n",
      "Recall: 1.0\n",
      "F1 Score: 0.871139510117146\n",
      "Precision: 0.7716981132075472\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[409   0 121   2]\n"
     ]
    }
   ],
   "source": [
    "# KNN model using SMOTE\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "knn.fit(X_train_smote, y_train_smote)\n",
    "y_pred_knn_smote = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_knn_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our KNN model performed better than our baseline model in almost every metric, except Recall that had a slidely drop. We will now run a GridSearchCV to find the most relevant hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_knn = {\n",
    "    'n_neighbors':list(range(1,10)),\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['eucliean','manhattan','minkowski'],\n",
    "    'leaf_size':list(range(1,101))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12000 candidates, totalling 60000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1794 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done 3048 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5280 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done 7480 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10376 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16376 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 23176 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 30776 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 35136 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 42680 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 52680 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.768796992481203\n",
      "Recall: 1.0\n",
      "F1 Score: 0.869287991498406\n",
      "Precision: 0.768796992481203\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[409   0 123   0]\n",
      "Best Parameters: {'leaf_size': 1, 'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'uniform'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using GridSearchCV for a KNN model\n",
    "gs_knn = GridSearchCV(knn,grid_params_knn,verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fitting and predicting\n",
    "gs_knn.fit(X_train_smote, y_train_smote)\n",
    "y_pred_gs_knn = gs_knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_gs_knn)\n",
    "print_results(gs_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier(n_neighbors=6,weights='distance',metric='manhattan',leaf_size=1)\n",
    "# knn.fit(X_train_scaled, y_train)\n",
    "# y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluation metrics\n",
    "# evaluation(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9398496240601504\n",
      "Recall: 0.9877750611246944\n",
      "F1 Score: 0.961904761904762\n",
      "Precision: 0.9373549883990719\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[404   5  27  96]\n"
     ]
    }
   ],
   "source": [
    "evaluationation(y_test, y_pred_knn_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our order of models, we will now try a Decision Tree model. On the baseline model, Decision Tree did not perform better in most of the metrics. However, it did perform better with the Precision metric, the focus of our project. We will run a model using only SMOTE and see the performance. We believe that Random Forest, since it's a ensemble model, will perform better than KNN. However, we want to see how the model perform without the imbalance classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9473684210526315\n",
      "Recall: 0.9755501222493888\n",
      "F1 Score: 0.9661016949152542\n",
      "Precision: 0.9568345323741008\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[399  10  18 105]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "tree.fit(X_train_smote, y_train_smote)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "Our Decision Tree model had a good performance. So far, it was our best performing model for precision metric. We expect to improve this result using Random Forest models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was our best performing baseline model. We will see if we can improve even more the results using hyperparamenter tuning. However, let's first see how the model performs using only SMOTE first. Then we will use GridSearchCV to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.956766917293233\n",
      "Recall: 0.9828850855745721\n",
      "F1 Score: 0.9721886336154776\n",
      "Precision: 0.9617224880382775\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[402   7  16 107]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "rfc.fit(X_train_smote, y_train_smote)\n",
    "y_preds_rfr = rfc.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_preds_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings: </b>\n",
    "\n",
    "Compared to our baseline model, we can see improvements already in all the metrics. We can also see that our model was able to reduced the number of False Positives, which is our focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to find the best hyperparameters for Random Forest, the model that interests us the most for now. First, we will set up a dictionary of hyperparameters that we want to try, then run a GridSearchCV to find the best fitting for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch Parameters\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 100, 150, 200],\n",
    "    'max_depth': list(range(1, 11)),\n",
    "    'criterion':['gini','entropy'],\n",
    "    'max_features': list(range(20)),\n",
    "    'oob_score':[False,True],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4000 candidates, totalling 20000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 18034 tasks      | elapsed: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed: 40.4min\n",
      "[Parallel(n_jobs=-1)]: Done 20000 out of 20000 | elapsed: 40.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9661654135338346\n",
      "Recall: 0.9853300733496333\n",
      "F1 Score: 0.9781553398058253\n",
      "Precision: 0.9710843373493976\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[403   6  12 111]\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 11, 'n_estimators': 150, 'oob_score': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # GridSearch (----------remove hyphen from GridSearchCV----------)\n",
    "cv = GridSearchCV(rfc, parameters, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "cv.fit(X_train_smote, y_train_smote)\n",
    "y_preds_rfr_cv = cv.predict((X_test))\n",
    "\n",
    "evaluation(y_test, y_preds_rfr_cv)\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great improvement from our baseline model. All the metrics improved and this is the best performing so far before feature engineering.\n",
    "\n",
    "To confirm that this is the best performing model, we would like to play a little bit with the `max_feature` hyperparameter, since we noticed that sometimes GridSearchCV might not choose the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9680451127819549\n",
      "Recall: 0.9828850855745721\n",
      "F1 Score: 0.9792935444579781\n",
      "Precision: 0.9757281553398058\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[402   7  10 113]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Random Forest Model\n",
    "rfc = RandomForestClassifier(criterion='entropy',\n",
    "                             n_estimators= 150, \n",
    "                             max_depth=9, \n",
    "                             max_features=10,\n",
    "                             oob_score=True\n",
    "                            )\n",
    "\n",
    "# Fitting and predicting\n",
    "rfc.fit(X_train_smote, y_train_smote)\n",
    "y_preds_rfr_tuned = rfc.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_preds_rfr_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "As we predicted, GridSearchCV didn't give us the best `max_features` for precision. We are not sure why this happens, so we always try changing it a little bit to see what the result would look like. We also changed `max_depth=10` to `max_depth=9` and the metric precision also improved.\n",
    "\n",
    "However, we are trying to decrease False Positives. GridSearchCV with Random Forest gave us the lowest False Positives.\n",
    "\n",
    "Finally, we will use XGBoost to see if we can improve our Random Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our last try to improve our model will be XGBoost, which uses the priciples of Gradient Descent and Boosting together. We believe there is might a chance for us to improve our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9379699248120301\n",
      "Recall: 0.9511002444987775\n",
      "F1 Score: 0.9593094944512948\n",
      "Precision: 0.9676616915422885\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[389  20  13 110]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Fit XGBClassifier\n",
    "xgb.fit(X_train_smote, y_train_smote)\n",
    "y_pred_xg = xgb.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jack model\n",
    "xg_clf = XGBClassifier(objective ='binary:logistic')\n",
    "\n",
    "param_dict = {'n_estimators':[500, 1000, 1500],\n",
    "              'learning_rate':[0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth':[9,10,11,12,13],\n",
    "              'colsample_bytree':[0.5,0.45,0.4],\n",
    "              'min_child_weight':[1,2,3]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xg = GridSearchCV(estimator=xg_clf,\n",
    "                      param_grid=param_dict,\n",
    "                      scoring='precision',\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1,\n",
    "                      iid=False,\n",
    "                      cv=10)\n",
    "\n",
    "grid_xg.fit(X_train_smote, y_train_smote)\n",
    "y_pred_grid_xg = grid_xg.predict(X_test)\n",
    "\n",
    "evaluation(y_test, y_pred_xg_clf_baseline)\n",
    "print_results(y_pred_grid_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "rf = RandomForestClassifier()\n",
    "scores = cross_val_score(rf, X_train_smote, y_train_smote, cv=3, scoring='precision')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [6],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = GridSearchCV(clf,param_grid,scoring='accuracy',cv=None,n_jobs=1,verbose=-1)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "y_pred_grid_clf = grid_clf.predict(X_test)\n",
    "\n",
    "evaluation(y_test, y_pred_grid_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GridSearch Parameters\n",
    "parameters_lr = {'warm_start':[True, False],\n",
    "                 'C':[.001,.01,.01,10,100],\n",
    "\n",
    "}\n",
    "\n",
    "grid_xg = GridSearchCV(estimator=lr,\n",
    "                      param_grid=parameters_lr,\n",
    "                      scoring='precision',\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1,\n",
    "                      cv=10)\n",
    "\n",
    "grid_xg.fit(X_train_smote, y_train_smote)\n",
    "y_pred_grid_xg_lr = grid_xg.predict(X_test)\n",
    "\n",
    "evaluation(y_test, y_pred_grid_xg_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
