{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run our baseline models to understand better our dataset and how the features and target variables behave when running different models. Our focus is to find baseline models that can improve the Precision without affecting Recall. To evaluate the models, we will use the metrics Precision, Accuracy, Recall, F1 Score, and Confusion Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, precision_score, confusion_matrix, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import set_config\n",
    "set_config(print_changed_only=False)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "% matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ead in UCI Heart Disiease Databasae\n",
    "df = pd.read_csv('fetal_health.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fetal_health = np.where(df.fetal_health > 1.0, 2.0, df.fetal_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Objectives: </b>\n",
    "\n",
    "- Assign our feature variables and target variable into the X and y variables. \n",
    "- Split our dataset using Sklearn Train Test Split. We will use the Sklearn default option and assign 75% to our train set and 25% to our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning features variables (X) and target variable (y)\n",
    "X = df.drop('fetal_health', axis=1)\n",
    "y = df.fetal_health\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Checking if train test split ran correclty\n",
    "for dataset in [y_train, y_test]:\n",
    "    print(round(len(dataset)/len(y), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train and test set were split correctly. 75% of the dataset was assigned to our train set, and 25% was assigned to our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand our dataset and how our features variables affect our target variable, we decided to run baseline models and find what approach we should take when tuning our models. None of the baseline models will have any kinds of hyperparameters tuning. We will use the default hyperparameters.\n",
    "\n",
    "<b>Objectives:</b>\n",
    "- Run a Logistic Regression, KNN, Decision Tree, and Random Forest model.\n",
    "- Create a function to simplify our model evaluation.\n",
    "- Evaluate each model using the Accuracy, Recall, F1 Score, and Precision metrics.\n",
    "- Create confusion matrices for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "    \n",
    "# Print Accuracy, Recall, F1 Score, and Precision metrics.\n",
    "    print('Evaluation Metrics:')\n",
    "    print('Accuracy: ' + str(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('Recall: ' + str(metrics.recall_score(y_test, y_pred)))\n",
    "    print('F1 Score: ' + str(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: ' + str(metrics.precision_score(y_test, y_pred)))\n",
    "    \n",
    "# Print confusion Matrix\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(' TN,  FP, FN, TP')\n",
    "    print(confusion_matrix(y_true, y_pred).ravel())\n",
    "    \n",
    "# Function prints best parameters for GridSearchCV\n",
    "def print_results(results):\n",
    "    print('Best Parameters: {}\\n'.format(results.best_params_))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to classify structured data, logistic regression models usually give a quick and reliable result. Thus, it will be our first baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8834586466165414\n",
      "Recall: 0.9535452322738386\n",
      "F1 Score: 0.9263657957244655\n",
      "Precision: 0.9006928406466512\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[390  19  43  80]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Logistic Regression Model\n",
    "\n",
    "lr_baseline = LogisticRegression()\n",
    "\n",
    "# Fitting and predicting\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "y_pred_lr_baseline = lr_baseline.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_lr_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "As we can see, our baseline Logistic Regression model performed reasonably well, even without tuning. Our feature variables can train the model with fairly precision.\n",
    "\n",
    "We will now test the same features and target variables using K-nearest neighbors, which classifies data points based on similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run a K-Nearest Neighbors model. KNN is a simples model that stores and classifies all data points based on similarity measures (i.e., distance functions). Since it's an easy model to set up, we will run it and read the results to understand what the data tells us. We will have to scale our dataset for KNN before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call and fit scaler \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scaling our dataset\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9398496240601504\n",
      "Recall: 0.9877750611246944\n",
      "F1 Score: 0.961904761904762\n",
      "Precision: 0.9373549883990719\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[404   5  27  96]\n"
     ]
    }
   ],
   "source": [
    "# Baseline KNN Model\n",
    "knn_baseline = KNeighborsClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "knn_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred_knn_baseline = knn_baseline.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_knn_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "Our baseline KNN model performed better than our baseline Logistic Regression model in all the metrics that we are analyzing the model. Interestingly, the Recall metric had a performance of .99. This is very high. It means that our baseline model is capable of predicting true positives with high precision. For now, we will keep running baseline models to find what the dataset has to tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our project's key questions is to find what metrics can predict which features have the highest coefficient in our model predictions. For this reason, we will run Decision Tree as well, since it has a better division of features to predict an outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Decision Tree Model\n",
    "dt_baseline = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "dt_baseline = tree_baseline.fit(X_train, y_train)\n",
    "y_pred_dt_baseline = tree_baseline.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_dt_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "Decision Tree did not perform better than KNN for most of the metrics that we are using. However, it performed better for the precision metric, which is the metric that we are most concerned about. Thus, we will run a Random Forest model, which works as a collection of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a Decision Tree model, it makes sense to run a Random Forest model, an ensemble model that operates by constructing a multitude of decision trees at training time and outputting the class that is the average prediction for the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9473684210526315\n",
      "Recall: 0.9902200488997555\n",
      "F1 Score: 0.9665871121718378\n",
      "Precision: 0.9440559440559441\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[405   4  24  99]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Random Forest Model\n",
    "rfc_baseline = RandomForestClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "rfc_baseline.fit(X_train, y_train)\n",
    "y_pred_rfr_baseline = rfc_baseline.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_rfr_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "Our baseline random forest model had an exciting performance. It outperformed the Decision Tree baseline model in almost every metric, except Precision, which slightly decreased. Compared to KNN, our best performing model so far, if we compare every metric, performs better for the metrics Accuracy, F1 Score, and Precision. It performed the same for the Recall metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Recall, and F1 Score. Decision Tree had the best performance for the Precision metric. All of our models performed reasonably well. However, we have space for improvement in all of them. We will start improving some hyperparameters and check how they performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a class imbalance problem in our dataset. Thus, we will use Synthetic Minority Oversampling Technique (SMOTE), where the minority class is oversampled by producing synthetic examples to fix the class imbalance. We will then compare the results to our baseline models to check if the target variable is even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    0.781681\n",
      "2.0    0.218319\n",
      "Name: fetal_health, dtype: float64\n",
      "2.0    0.5\n",
      "1.0    0.5\n",
      "Name: fetal_health, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "# Checking if SMOTE was correctly fitted\n",
    "for dataset in (y_train, y_train_smote):\n",
    "    print (dataset.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Models Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start improving our model using hyperparameters tuning without any feature engineering. We will tune all the baseline models changing some hyperparameters. Then, we will use GridSearchCV and XGBoost to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the first baseline model that we ran: Logistic Regression using SMOTE. We want to see if SMOTE can help our model best predict our target variable. We will not tune any hyperparameter for the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8402255639097744\n",
      "Recall: 0.8410757946210269\n",
      "F1 Score: 0.8900388098318239\n",
      "Precision: 0.945054945054945\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[344  65  20 103]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fitting and predicting\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "y_pred_lr_smote = lr.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_lr_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to our model using SMOTE underperformed in almost every metric using SMOTE. However, the Precision metric did perform better. It's the primary metric that we are interested in. We can see that there the number of False Negatives was reduced.\n",
    "\n",
    "Next, we will add a few hyperparameters individually and see if we can improve our model. Here, our approach will be trying random hyperparameters and check the results compared to the model without any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8402255639097744\n",
      "Recall: 0.8410757946210269\n",
      "F1 Score: 0.8900388098318239\n",
      "Precision: 0.945054945054945\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[344  65  20 103]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "lr = LogisticRegression(C=100, max_iter=200, class_weight='balanced')\n",
    "\n",
    "# Fitting and predicting\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "y_pred_lr_tuned = lr.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_lr_tuned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`C=100` improved the Precision metric. All the other metrics remained the same.\n",
    "\n",
    "`fit_intercept=False` reduced all the metrics.\n",
    "\n",
    "`max_inter=200` improved the Precision metric compared to the default `max_inter=100`\n",
    "\n",
    "`class_weight='balanced'` improved all the metrics.\n",
    "\n",
    "<b>Findings:</b>\n",
    "\n",
    "Compared to our baseline model, the Logistic Regression model did not perform better. We were able to improve the model slightly using some random tuning. However, we don't think that Logistic Regression has the power that we need. Thus, we will use some more advanced algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our order of baseline models, we will know use KNN models. We will first compare the result of our baseline model to a model after applying SMOTE. Then, we will use GridSearchCV to find the best hyperparameter tuning. For this model, we will scale the train set using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.75\n",
      "Recall: 0.9070904645476773\n",
      "F1 Score: 0.848\n",
      "Precision: 0.796137339055794\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[371  38  95  28]\n"
     ]
    }
   ],
   "source": [
    "# KNN model using SMOTE\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "knn.fit(X_train_smote, y_train_smote)\n",
    "y_pred_knn_smote = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_knn_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our KNN model performed better than our baseline model in almost every metric, except Recall that had a small drop. We will now run a GridSearchCV to find the most relevant hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_knn = {\n",
    "    'n_neighbors':list(range(1,10)),\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['eucliean','manhattan','minkowski'],\n",
    "    'leaf_size':list(range(1,101))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5400 candidates, totalling 27000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2976 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 5776 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9376 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 13776 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 18976 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 24976 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.768796992481203\n",
      "Recall: 1.0\n",
      "F1 Score: 0.869287991498406\n",
      "Precision: 0.768796992481203\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[409   0 123   0]\n",
      "Best Parameters: {'leaf_size': 1, 'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'uniform'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 27000 out of 27000 | elapsed:  2.3min finished\n"
     ]
    }
   ],
   "source": [
    "# Using GridSearchCV for a KNN model\n",
    "gs_knn = GridSearchCV(knn,grid_params_knn,verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fitting and predicting\n",
    "gs_knn.fit(X_train_smote, y_train_smote)\n",
    "y_pred_gs_knn = gs_knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_gs_knn)\n",
    "print_results(gs_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN using GridSearchCV did not perform better than our baseline model using only SMOTE. The Recall metric increased to 1.0, which might be a sign of overfitting. The model is not capable of identify False Positives or True Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our order of models, we will now try a Decision Tree model. On the baseline model, Decision Tree did not perform better in most of the metrics. However, it did perform better with the Precision metric, the focus of our project. We will run a model using only SMOTE and see the performance. We believe that Random Forest, since it's an ensemble model, will perform better than KNN. However, we want to know how the model performs without the imbalance classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9304511278195489\n",
      "Recall: 0.960880195599022\n",
      "F1 Score: 0.9550425273390036\n",
      "Precision: 0.9492753623188406\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[393  16  21 102]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "tree.fit(X_train_smote, y_train_smote)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluation(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "Our Decision Tree model had a good performance. So far, it was our best performing model for precision metric. We expect to improve this result using Random Forest models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was our best performing baseline model. We will see if we can improve even more the results using hyperparameter tuning. However, let's first see how the model performs using only SMOTE first. Then we will use GridSearchCV to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9511278195488722\n",
      "Recall: 0.9828850855745721\n",
      "F1 Score: 0.9686746987951808\n",
      "Precision: 0.9548693586698337\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[402   7  19 104]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "rfc.fit(X_train_smote, y_train_smote)\n",
    "y_preds_rfr = rfc.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_preds_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings: </b>\n",
    "\n",
    "Compared to our baseline model, we can see improvements already in all the metrics. We can also see that our model reduced the number of False Positives, which is our focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to find the best hyperparameters for Random Forest, the model that interests us the most for now. First, we will set up a dictionary of hyperparameters that we want to try and then run a GridSearchCV to find the best fit for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch Parameters\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 100, 150, 200],\n",
    "    'max_depth': list(range(1, 11)),\n",
    "    'criterion':['gini','entropy'],\n",
    "    'max_features': list(range(20)),\n",
    "    'oob_score':[False,True],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4000 candidates, totalling 20000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 18034 tasks      | elapsed: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed: 40.4min\n",
      "[Parallel(n_jobs=-1)]: Done 20000 out of 20000 | elapsed: 40.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9661654135338346\n",
      "Recall: 0.9853300733496333\n",
      "F1 Score: 0.9781553398058253\n",
      "Precision: 0.9710843373493976\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[403   6  12 111]\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 11, 'n_estimators': 150, 'oob_score': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # GridSearch (----------remove hyphen from GridSearchCV----------)\n",
    "rfc_gs = GridSearchCV(rfc, parameters, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "rfc_gs.fit(X_train_smote, y_train_smote)\n",
    "y_preds_rfr_cv = rfc_gs.predict((X_test))\n",
    "\n",
    "evaluation(y_test, y_preds_rfr_rfc_gs)\n",
    "print_results(rfc_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great improvement from our baseline model. All the metrics improved and this is the best performing so far before feature engineering.\n",
    "\n",
    "To confirm that this is the best performing model, we would like to play a little bit with the `max_feature` hyperparameter since we noticed that sometimes GridSearchCV might not choose the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9605263157894737\n",
      "Recall: 0.9779951100244498\n",
      "F1 Score: 0.9744214372716199\n",
      "Precision: 0.970873786407767\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[400   9  12 111]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Random Forest Model\n",
    "rfc = RandomForestClassifier(criterion='entropy',\n",
    "                             n_estimators= 150, \n",
    "                             max_depth=9, \n",
    "                             max_features=10,\n",
    "                             oob_score=True\n",
    "                            )\n",
    "\n",
    "# Fitting and predicting\n",
    "rfc.fit(X_train_smote, y_train_smote)\n",
    "y_preds_rfr_tuned = rfc.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_preds_rfr_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "As we predicted, GridSearchCV didn't give us the best `max_features` for precision. We are not sure why this happens, so we always try changing it slightly to see what the result would look like. We changed `max_depth=10` to `max_depth=9` and the metric precision improved.\n",
    "\n",
    "However, we are trying to decrease False Positives. GridSearchCV with Random Forest gave us the lowest False Positives.\n",
    "\n",
    "Finally, we will use XGBoost to see if we can improve our Random Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our last try to improve our model will be XGBoost, which uses Gradient Descent and Boosting principles. We believe there might have space for improvement. First, we will use the default hyperparameters. Then, we will use XGBoost with GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.9454887218045113\n",
      "Recall: 0.9633251833740831\n",
      "F1 Score: 0.9645042839657283\n",
      "Precision: 0.9656862745098039\n",
      "\n",
      "Confusion Matrix:\n",
      " TN,  FP, FN, TP\n",
      "[394  15  14 109]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Fitting and predicting\n",
    "xgb.fit(X_train_smote, y_train_smote)\n",
    "y_pred_xg = xgb.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation(y_test, y_pred_xg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Findings:</b>\n",
    "\n",
    "XGBoost performed using only the default parameters. It did not perform better than Random Forest using Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
